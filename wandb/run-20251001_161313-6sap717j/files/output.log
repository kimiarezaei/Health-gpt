[34m[1mwandb[0m: Detected [huggingface_hub.inference] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2634/2634 [02:36<00:00, 16.88it/s]
{'loss': 2.0997, 'grad_norm': 2.7527060508728027, 'learning_rate': 4.8234624145785876e-05, 'epoch': 0.11}
{'loss': 1.3009, 'grad_norm': 3.53792667388916, 'learning_rate': 4.633637053910403e-05, 'epoch': 0.23}
{'loss': 1.309, 'grad_norm': 3.333481788635254, 'learning_rate': 4.443811693242218e-05, 'epoch': 0.34}
{'loss': 1.2841, 'grad_norm': 3.355242967605591, 'learning_rate': 4.253986332574032e-05, 'epoch': 0.46}
{'loss': 1.1735, 'grad_norm': 1.8551448583602905, 'learning_rate': 4.0641609719058465e-05, 'epoch': 0.57}
{'loss': 1.1514, 'grad_norm': 2.7187771797180176, 'learning_rate': 3.874335611237662e-05, 'epoch': 0.68}
{'loss': 1.0442, 'grad_norm': 3.867370128631592, 'learning_rate': 3.684510250569476e-05, 'epoch': 0.8}
{'loss': 1.006, 'grad_norm': 2.8036112785339355, 'learning_rate': 3.4946848899012906e-05, 'epoch': 0.91}
{'loss': 0.9856, 'grad_norm': 2.388848304748535, 'learning_rate': 3.304859529233106e-05, 'epoch': 1.03}
{'loss': 0.9353, 'grad_norm': 3.241215944290161, 'learning_rate': 3.11503416856492e-05, 'epoch': 1.14}
{'loss': 0.9331, 'grad_norm': 3.174381732940674, 'learning_rate': 2.9252088078967348e-05, 'epoch': 1.25}
{'loss': 0.91, 'grad_norm': 2.785736322402954, 'learning_rate': 2.73538344722855e-05, 'epoch': 1.37}
{'loss': 0.8708, 'grad_norm': 3.460829257965088, 'learning_rate': 2.5455580865603646e-05, 'epoch': 1.48}
{'loss': 0.8911, 'grad_norm': 3.0579731464385986, 'learning_rate': 2.3557327258921793e-05, 'epoch': 1.59}
{'loss': 0.8579, 'grad_norm': 2.2534799575805664, 'learning_rate': 2.165907365223994e-05, 'epoch': 1.71}
{'loss': 0.8116, 'grad_norm': 2.7506253719329834, 'learning_rate': 1.9760820045558088e-05, 'epoch': 1.82}
{'loss': 0.7995, 'grad_norm': 1.6320589780807495, 'learning_rate': 1.7862566438876235e-05, 'epoch': 1.94}
{'loss': 0.7722, 'grad_norm': 1.201439380645752, 'learning_rate': 1.5964312832194382e-05, 'epoch': 2.05}
{'loss': 0.7448, 'grad_norm': 3.290536642074585, 'learning_rate': 1.4066059225512529e-05, 'epoch': 2.16}
{'loss': 0.7265, 'grad_norm': 3.866461992263794, 'learning_rate': 1.2167805618830676e-05, 'epoch': 2.28}
{'loss': 0.8289, 'grad_norm': 3.1791415214538574, 'learning_rate': 1.0269552012148824e-05, 'epoch': 2.39}
{'loss': 0.7227, 'grad_norm': 2.647451162338257, 'learning_rate': 8.37129840546697e-06, 'epoch': 2.51}
{'loss': 0.7396, 'grad_norm': 1.8247146606445312, 'learning_rate': 6.473044798785117e-06, 'epoch': 2.62}
{'loss': 0.7033, 'grad_norm': 1.5480186939239502, 'learning_rate': 4.574791192103265e-06, 'epoch': 2.73}
{'loss': 0.7564, 'grad_norm': 3.092301845550537, 'learning_rate': 2.6765375854214124e-06, 'epoch': 2.85}
{'loss': 0.7252, 'grad_norm': 2.8391976356506348, 'learning_rate': 7.782839787395596e-07, 'epoch': 2.96}
{'train_runtime': 167.6799, 'train_samples_per_second': 62.834, 'train_steps_per_second': 15.709, 'train_loss': 0.9629401212039099, 'epoch': 3.0}
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
C:\Users\krezaei\AppData\Roaming\Python\Python311\site-packages\transformers\generation\utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.
  warnings.warn(
Traceback (most recent call last):
  File "c:\Users\krezaei\OneDrive - University College Cork\github\text generator using gpt\main.py", line 55, in <module>
    outputs = model.generate(**inputs, max_length=50)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\.conda\envs\vasie_kimia\Lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\AppData\Roaming\Python\Python311\site-packages\transformers\generation\utils.py", line 1576, in generate
    result = self._greedy_search(
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\AppData\Roaming\Python\Python311\site-packages\transformers\generation\utils.py", line 2494, in _greedy_search
    outputs = self(
              ^^^^^
  File "C:\Users\krezaei\.conda\envs\vasie_kimia\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\.conda\envs\vasie_kimia\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\AppData\Roaming\Python\Python311\site-packages\accelerate\utils\operations.py", line 822, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\AppData\Roaming\Python\Python311\site-packages\accelerate\utils\operations.py", line 810, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\.conda\envs\vasie_kimia\Lib\site-packages\torch\amp\autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\AppData\Roaming\Python\Python311\site-packages\transformers\models\gpt2\modeling_gpt2.py", line 1305, in forward
    transformer_outputs = self.transformer(
                          ^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\.conda\envs\vasie_kimia\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\.conda\envs\vasie_kimia\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\AppData\Roaming\Python\Python311\site-packages\transformers\models\gpt2\modeling_gpt2.py", line 1068, in forward
    inputs_embeds = self.wte(input_ids)
                    ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\.conda\envs\vasie_kimia\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\.conda\envs\vasie_kimia\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\krezaei\.conda\envs\vasie_kimia\Lib\site-packages\torch\nn\modules\sparse.py", line 163, in forward
    return F.embedding(
           ^^^^^^^^^^^^
  File "C:\Users\krezaei\.conda\envs\vasie_kimia\Lib\site-packages\torch\nn\functional.py", line 2264, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)
