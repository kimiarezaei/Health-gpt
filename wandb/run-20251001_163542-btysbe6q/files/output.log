[34m[1mwandb[0m: Detected [huggingface_hub.inference] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2634/2634 [02:37<00:00, 16.71it/s]
{'loss': 2.0997, 'grad_norm': 2.7527060508728027, 'learning_rate': 4.8234624145785876e-05, 'epoch': 0.11}
{'loss': 1.3009, 'grad_norm': 3.53792667388916, 'learning_rate': 4.633637053910403e-05, 'epoch': 0.23}
{'loss': 1.309, 'grad_norm': 3.333481788635254, 'learning_rate': 4.443811693242218e-05, 'epoch': 0.34}
{'loss': 1.2841, 'grad_norm': 3.355242967605591, 'learning_rate': 4.253986332574032e-05, 'epoch': 0.46}
{'loss': 1.1735, 'grad_norm': 1.8551448583602905, 'learning_rate': 4.0641609719058465e-05, 'epoch': 0.57}
{'loss': 1.1514, 'grad_norm': 2.7187771797180176, 'learning_rate': 3.874335611237662e-05, 'epoch': 0.68}
{'loss': 1.0442, 'grad_norm': 3.867370128631592, 'learning_rate': 3.684510250569476e-05, 'epoch': 0.8}
{'loss': 1.006, 'grad_norm': 2.8036112785339355, 'learning_rate': 3.4946848899012906e-05, 'epoch': 0.91}
{'loss': 0.9856, 'grad_norm': 2.388848304748535, 'learning_rate': 3.304859529233106e-05, 'epoch': 1.03}
{'loss': 0.9353, 'grad_norm': 3.241215944290161, 'learning_rate': 3.11503416856492e-05, 'epoch': 1.14}
{'loss': 0.9331, 'grad_norm': 3.174381732940674, 'learning_rate': 2.9252088078967348e-05, 'epoch': 1.25}
{'loss': 0.91, 'grad_norm': 2.785736322402954, 'learning_rate': 2.73538344722855e-05, 'epoch': 1.37}
{'loss': 0.8708, 'grad_norm': 3.460829257965088, 'learning_rate': 2.5455580865603646e-05, 'epoch': 1.48}
{'loss': 0.8911, 'grad_norm': 3.0579731464385986, 'learning_rate': 2.3557327258921793e-05, 'epoch': 1.59}
{'loss': 0.8579, 'grad_norm': 2.2534799575805664, 'learning_rate': 2.165907365223994e-05, 'epoch': 1.71}
{'loss': 0.8116, 'grad_norm': 2.7506253719329834, 'learning_rate': 1.9760820045558088e-05, 'epoch': 1.82}
{'loss': 0.7995, 'grad_norm': 1.6320589780807495, 'learning_rate': 1.7862566438876235e-05, 'epoch': 1.94}
{'loss': 0.7722, 'grad_norm': 1.201439380645752, 'learning_rate': 1.5964312832194382e-05, 'epoch': 2.05}
{'loss': 0.7448, 'grad_norm': 3.290536642074585, 'learning_rate': 1.4066059225512529e-05, 'epoch': 2.16}
{'loss': 0.7265, 'grad_norm': 3.866461992263794, 'learning_rate': 1.2167805618830676e-05, 'epoch': 2.28}
{'loss': 0.8289, 'grad_norm': 3.1791415214538574, 'learning_rate': 1.0269552012148824e-05, 'epoch': 2.39}
{'loss': 0.7227, 'grad_norm': 2.647451162338257, 'learning_rate': 8.37129840546697e-06, 'epoch': 2.51}
{'loss': 0.7396, 'grad_norm': 1.8247146606445312, 'learning_rate': 6.473044798785117e-06, 'epoch': 2.62}
{'loss': 0.7033, 'grad_norm': 1.5480186939239502, 'learning_rate': 4.574791192103265e-06, 'epoch': 2.73}
{'loss': 0.7564, 'grad_norm': 3.092301845550537, 'learning_rate': 2.6765375854214124e-06, 'epoch': 2.85}
{'loss': 0.7252, 'grad_norm': 2.8391976356506348, 'learning_rate': 7.782839787395596e-07, 'epoch': 2.96}
{'train_runtime': 162.4753, 'train_samples_per_second': 64.847, 'train_steps_per_second': 16.212, 'train_loss': 0.9629401212039099, 'epoch': 3.0}
C:\Users\krezaei\AppData\Roaming\Python\Python311\site-packages\transformers\generation\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The reason for your depression may be that I am not as good as I thought I was. Iâ€™ve never tried drugs, but Iâ€™ve been smoking crack. Iâ€™ve never tried to help myself, but Iâ€™
